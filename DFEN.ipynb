{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Memeriksa apakah GPU tersedia dan PyTorch menggunakan GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dimas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Konfigurasi GPU jika tersedia\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 12.1\n",
      "ID of current CUDA device: 0\n",
      "Name of current CUDA device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 raw               sentids  \\\n",
      "0  [\"Two young guys with shaggy hair look at thei...       [0, 1, 2, 3, 4]   \n",
      "1  [\"Several men in hard hats are operating a gia...       [5, 6, 7, 8, 9]   \n",
      "2  [\"A child in a pink dress is climbing up a set...  [10, 11, 12, 13, 14]   \n",
      "3  [\"Someone in a blue shirt and hat is standing ...  [15, 16, 17, 18, 19]   \n",
      "4  [\"Two men, one in a gray shirt, one in a black...  [20, 21, 22, 23, 24]   \n",
      "\n",
      "   split        filename  img_id  \n",
      "0  train  1000092795.jpg       0  \n",
      "1  train    10002456.jpg       1  \n",
      "2  train  1000268201.jpg       2  \n",
      "3  train  1000344755.jpg       3  \n",
      "4  train  1000366164.jpg       4  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file to see its structure\n",
    "csv_file = \"./dataset/data_caption/flickr_annotations_30k.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset dan Dataloader\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = self._create_image_caption_pairs()\n",
    "\n",
    "    def _create_image_caption_pairs(self):\n",
    "        data = []\n",
    "        for idx in range(len(self.annotations)):\n",
    "            img_name = self.annotations.iloc[idx][\"filename\"]\n",
    "            captions = self.annotations.iloc[idx][\"raw\"].split(\n",
    "                \",\"\n",
    "            )  # Split the captions by comma\n",
    "            for caption in captions:\n",
    "                data.append((img_name, caption.strip()))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.data[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tokens = word_tokenize(caption.lower())\n",
    "\n",
    "        return image, tokens\n",
    "\n",
    "\n",
    "# Transformasi untuk dataset\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Membuat objek dataset dan memisahkan dataset menjadi train dan val\n",
    "csv_file = \"./dataset/data_caption/flickr_annotations_30k.csv\"  # Ganti dengan path file CSV Anda\n",
    "root_dir = (\n",
    "    \"./dataset/data_image/flickr30k-images/\"  # Ganti dengan path direktori gambar Anda\n",
    ")\n",
    "\n",
    "dataset = Flickr30kDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
    "train_indices, val_indices = train_test_split(\n",
    "    list(range(len(dataset))), test_size=0.2, random_state=42\n",
    ")\n",
    "train_set = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_set = torch.utils.data.Subset(dataset, val_indices)\n",
    "\n",
    "# Membuat DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Membuat vocab\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # Awal dari vocab dengan token spesial\n",
    "for idx, token in enumerate(\n",
    "    set(word_tokenize(\" \".join(dataset.annotations[\"raw\"].values).lower())), 2\n",
    "):\n",
    "    vocab[token] = idx\n",
    "\n",
    "\n",
    "# Fungsi untuk mengonversi token caption menjadi tensor\n",
    "def tokens_to_tensor(tokens, vocab):\n",
    "    indices = [vocab[token] if token in vocab else vocab[\"<UNK>\"] for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Fungsi untuk batch collate\n",
    "def collate_fn(batch):\n",
    "    images, captions = zip(*batch)\n",
    "    max_len = max(len(caption) for caption in captions)\n",
    "    captions_padded = [\n",
    "        tokens_to_tensor(caption, vocab).tolist()\n",
    "        + [vocab[\"<PAD>\"]] * (max_len - len(caption))\n",
    "        for caption in captions\n",
    "    ]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    captions_padded = torch.tensor(captions_padded, dtype=torch.long)\n",
    "    return images, captions_padded\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=32, shuffle=True, num_workers=4, collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=32, shuffle=False, num_workers=4, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model DFEN\n",
    "class ImageEnhance(nn.Module):\n",
    "    def __init__(self, resnet_model):\n",
    "        super(ImageEnhance, self).__init__()\n",
    "        self.resnet = resnet_model\n",
    "        self.conv3 = nn.Sequential(*list(resnet_model.children())[:5])\n",
    "        self.conv4 = nn.Sequential(*list(resnet_model.children())[5])\n",
    "        self.conv5 = nn.Sequential(*list(resnet_model.children())[6])\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 256), nn.ReLU(), nn.Linear(256, 512), nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv3_features = self.conv3(x)\n",
    "        conv4_features = self.conv4(conv3_features)\n",
    "        conv5_features = self.conv5(conv4_features)\n",
    "        attention_weights = self.attention(\n",
    "            conv5_features.view(conv5_features.size(0), -1)\n",
    "        )\n",
    "        enhanced_features = attention_weights * conv5_features.view(\n",
    "            conv5_features.size(0), -1\n",
    "        )\n",
    "        return enhanced_features\n",
    "\n",
    "\n",
    "class TextEnhance(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(TextEnhance, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        outputs = self.fc(lstm_out)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class DFEN(nn.Module):\n",
    "    def __init__(self, resnet_model, vocab_size, embed_size, hidden_size):\n",
    "        super(DFEN, self).__init__()\n",
    "        self.image_enhance = ImageEnhance(resnet_model)\n",
    "        self.text_enhance = TextEnhance(vocab_size, embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        enhanced_features = self.image_enhance(images)\n",
    "        outputs = self.text_enhance(enhanced_features, captions)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Inisialisasi model dengan ResNet dan parameter lainnya\n",
    "resnet_model = models.resnet50()\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "model = DFEN(resnet_model, vocab_size, embed_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pelatihan Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, captions) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "        # Backward pass dan optimasi\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    bleu_scores = []\n",
    "    meteor_scores = []\n",
    "    rouge_scores = []\n",
    "    cider_scores = []\n",
    "    spice_scores = []\n",
    "\n",
    "    cider_scorer = Cider()\n",
    "    spice_scorer = Spice()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            outputs = model(images, captions)\n",
    "            predicted_captions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                reference = captions[i].cpu().numpy()\n",
    "                hypothesis = predicted_captions[i].cpu().numpy()\n",
    "\n",
    "                # Konversi numpy array ke list of tokens\n",
    "                reference = [str(x) for x in reference]\n",
    "                hypothesis = [str(x) for x in hypothesis]\n",
    "\n",
    "                # BLEU\n",
    "                bleu_score = sentence_bleu([reference], hypothesis)\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "                # METEOR\n",
    "                meteor = meteor_score([\" \".join(reference)], \" \".join(hypothesis))\n",
    "                meteor_scores.append(meteor)\n",
    "\n",
    "                # ROUGE\n",
    "                rouge_scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "                rouge_score = rouge_scorer.score(\n",
    "                    \" \".join(reference), \" \".join(hypothesis)\n",
    "                )[\"rougeL\"].fmeasure\n",
    "                rouge_scores.append(rouge_score)\n",
    "\n",
    "                # CIDEr dan SPICE\n",
    "                cider_score, _ = cider_scorer.compute_score(\n",
    "                    {i: [\" \".join(reference)]}, {i: [\" \".join(hypothesis)]}\n",
    "                )\n",
    "                cider_scores.append(cider_score)\n",
    "\n",
    "                spice_score, _ = spice_scorer.compute_score(\n",
    "                    {i: [\" \".join(reference)]}, {i: [\" \".join(hypothesis)]}\n",
    "                )\n",
    "                spice_scores.append(spice_score)\n",
    "\n",
    "    # Menghitung rata-rata skor untuk setiap metrik\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "    avg_cider = sum(cider_scores) / len(cider_scores)\n",
    "    avg_spice = sum(spice_scores) / len(spice_scores)\n",
    "\n",
    "    print(\n",
    "        f\"BLEU: {avg_bleu:.4f}, METEOR: {avg_meteor:.4f}, ROUGE: {avg_rouge:.4f}, CIDEr: {avg_cider:.4f}, SPICE: {avg_spice:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, val_loader, vocab, num_examples=5):\n",
    "    model.eval()\n",
    "    examples_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for images, captions in val_loader:\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            outputs = model(images, captions)\n",
    "            predicted_captions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                if examples_shown >= num_examples:\n",
    "                    break\n",
    "\n",
    "                reference = captions[i].cpu().numpy()\n",
    "                hypothesis = predicted_captions[i].cpu().numpy()\n",
    "\n",
    "                # Konversi numpy array ke list of tokens\n",
    "                reference = [\n",
    "                    idx_to_word(x, vocab) for x in reference if x != vocab[\"<PAD>\"]\n",
    "                ]\n",
    "                hypothesis = [\n",
    "                    idx_to_word(x, vocab) for x in hypothesis if x != vocab[\"<PAD>\"]\n",
    "                ]\n",
    "\n",
    "                # Menampilkan gambar dan caption\n",
    "                image = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                plt.imshow(image)\n",
    "                plt.title(\n",
    "                    f\"Prediction: {' '.join(hypothesis)}\\nGround Truth: {' '.join(reference)}\"\n",
    "                )\n",
    "                plt.axis(\"off\")\n",
    "                plt.show()\n",
    "\n",
    "                examples_shown += 1\n",
    "\n",
    "\n",
    "def idx_to_word(idx, vocab):\n",
    "    # Fungsi untuk mengonversi indeks menjadi kata\n",
    "    for word, index in vocab.items():\n",
    "        if index == idx:\n",
    "            return word\n",
    "    return \"<UNK>\"\n",
    "\n",
    "\n",
    "# Menampilkan beberapa prediksi dan ground truth dari dataset validasi\n",
    "show_predictions(model, val_loader, vocab, num_examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
